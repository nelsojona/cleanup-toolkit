name: Test Suite

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  COVERAGE_THRESHOLD: 80

jobs:
  # Unit Tests Job
  unit-tests:
    name: Unit Tests
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.9', '3.10', '3.11', '3.12']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements-test.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
    
    - name: Run unit tests
      run: |
        pytest tests/unit -v --cov=. --cov-report=xml --cov-report=html --junitxml=junit/test-results-unit.xml
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-unit-${{ matrix.os }}-py${{ matrix.python-version }}
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: unit-test-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: junit/test-results-unit.xml

  # Integration Tests Job
  integration-tests:
    name: Integration Tests
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
    
    - name: Install system dependencies
      run: |
        if [ "${{ runner.os }}" == "Linux" ]; then
          sudo apt-get update
          sudo apt-get install -y git gh shellcheck
        elif [ "${{ runner.os }}" == "macOS" ]; then
          brew install gh shellcheck
        fi
      shell: bash
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
    
    - name: Run integration tests
      run: |
        pytest tests/integration -v --cov=. --cov-report=xml --junitxml=junit/test-results-integration.xml
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: integration
        name: codecov-integration-${{ matrix.os }}
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: integration-test-results-${{ matrix.os }}
        path: junit/test-results-integration.xml

  # End-to-End Tests Job
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y git gh shellcheck
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
    
    - name: Run E2E tests
      run: |
        pytest tests/e2e -v --cov=. --cov-report=xml --junitxml=junit/test-results-e2e.xml --timeout=300
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: e2e
        name: codecov-e2e
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: e2e-test-results
        path: junit/test-results-e2e.xml

  # Shell Script Tests Job
  shell-tests:
    name: Shell Script Tests
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Install shellcheck
      run: |
        if [ "${{ runner.os }}" == "Linux" ]; then
          sudo apt-get update
          sudo apt-get install -y shellcheck
        elif [ "${{ runner.os }}" == "macOS" ]; then
          brew install shellcheck
        fi
      shell: bash
    
    - name: Run shellcheck on scripts
      run: |
        shellcheck hooks/pre-commit || true
        shellcheck scripts/*.sh || true
        shellcheck install.sh || true
    
    - name: Test installation script
      run: |
        # Test installation in a temporary directory
        temp_dir=$(mktemp -d)
        cd "$temp_dir"
        git init
        cp ${{ github.workspace }}/install.sh .
        echo "n" | bash install.sh || true
        # Check if toolkit directory was created
        ls -la .cleanup-toolkit/ || true
      shell: bash
    
    - name: Test pre-commit hook
      run: |
        # Test pre-commit hook execution
        temp_dir=$(mktemp -d)
        cd "$temp_dir"
        git init
        git config user.name "Test"
        git config user.email "test@example.com"
        
        # Copy hook
        mkdir -p .git/hooks
        cp ${{ github.workspace }}/hooks/pre-commit .git/hooks/
        chmod +x .git/hooks/pre-commit
        
        # Create test file and try to commit
        echo "print('debug')" > test.py
        git add test.py
        SKIP_CLEANUP=true git commit -m "Test" || true
      shell: bash

  # Security Tests Job
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety
    
    - name: Run Bandit security scan
      run: |
        bandit -r . -f json -o bandit-report.json || true
    
    - name: Run Safety check
      run: |
        safety check --json > safety-report.json || true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Performance Tests Job
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt
    
    - name: Run performance tests
      run: |
        pytest tests/performance -v --benchmark-only --benchmark-json=benchmark.json
      continue-on-error: true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark.json

  # Code Quality Job
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install linting tools
      run: |
        python -m pip install --upgrade pip
        pip install black isort flake8 pylint mypy
    
    - name: Run Black
      run: |
        black --check tests/ || true
    
    - name: Run isort
      run: |
        isort --check-only tests/ || true
    
    - name: Run Flake8
      run: |
        flake8 tests/ --count --select=E9,F63,F7,F82 --show-source --statistics || true
    
    - name: Run Pylint
      run: |
        pylint tests/ --exit-zero
    
    - name: Run MyPy
      run: |
        mypy tests/ --ignore-missing-imports || true

  # Coverage Report Job
  coverage-report:
    name: Coverage Report
    needs: [unit-tests, integration-tests, e2e-tests]
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install coverage tools
      run: |
        python -m pip install --upgrade pip
        pip install coverage
    
    - name: Generate coverage report
      run: |
        echo "# Coverage Report" > coverage-summary.md
        echo "Generated on: $(date)" >> coverage-summary.md
        echo "" >> coverage-summary.md
        echo "## Test Coverage Threshold: ${{ env.COVERAGE_THRESHOLD }}%" >> coverage-summary.md
    
    - name: Comment PR with coverage
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const coverageSummary = fs.readFileSync('coverage-summary.md', 'utf8');
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: coverageSummary
          });

  # Test Report Summary Job
  test-summary:
    name: Test Summary
    needs: [unit-tests, integration-tests, e2e-tests, shell-tests, security-tests, performance-tests]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3
    
    - name: Publish test results
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always()
      with:
        files: |
          **/test-results-*.xml
        check_name: Test Results
        comment_title: Test Results Summary
    
    - name: Create test summary
      run: |
        echo "# Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Results" >> $GITHUB_STEP_SUMMARY
        echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- E2E Tests: ${{ needs.e2e-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Shell Tests: ${{ needs.shell-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Security Tests: ${{ needs.security-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Performance Tests: ${{ needs.performance-tests.result }}" >> $GITHUB_STEP_SUMMARY